{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"9c6da53ade2302c5c2549050a755f7271f56c4989f0bdbb18e4ce6e64f091b09"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames[:3]:\n#         print(os.path.join(dirname, filename))\n#     if len(filenames) > 3:\n#         print(\"...\")\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-12-13T06:46:06.733786Z","iopub.execute_input":"2022-12-13T06:46:06.734767Z","iopub.status.idle":"2022-12-13T06:46:06.741268Z","shell.execute_reply.started":"2022-12-13T06:46:06.734714Z","shell.execute_reply":"2022-12-13T06:46:06.739700Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import csv\nimport cv2\nimport numpy as np\nimport random\nimport os\nimport time\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision.io import read_image\nimport torchvision.transforms as transforms","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.744695Z","iopub.execute_input":"2022-12-13T06:46:06.745639Z","iopub.status.idle":"2022-12-13T06:46:06.753079Z","shell.execute_reply.started":"2022-12-13T06:46:06.745601Z","shell.execute_reply":"2022-12-13T06:46:06.752086Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/captcha-hacker/train\"\nTEST_PATH = \"/kaggle/input/captcha-hacker/test\"\n#TRAIN_PATH = \"./train\"\n#TEST_PATH = \"./test\"\ndevice = torch.device(\"cuda\")\n# try device = \"cuda\" \n# and change your settings/accelerator to GPU if you want it to run faster","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.755096Z","iopub.execute_input":"2022-12-13T06:46:06.755534Z","iopub.status.idle":"2022-12-13T06:46:06.766230Z","shell.execute_reply.started":"2022-12-13T06:46:06.755499Z","shell.execute_reply":"2022-12-13T06:46:06.765144Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"code = {}\nrev_code = {}\n\nnum = 0\nfor i in range(10):\n    code[str(i)] = num\n    rev_code[num] = str(i)\n    num += 1\n\nfor i in range(ord('a'), ord('z') + 1):\n    code[chr(i)]  = num\n    rev_code[num] = chr(i)\n    num += 1\n\n#print(code)   \n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.769867Z","iopub.execute_input":"2022-12-13T06:46:06.770132Z","iopub.status.idle":"2022-12-13T06:46:06.778961Z","shell.execute_reply.started":"2022-12-13T06:46:06.770100Z","shell.execute_reply":"2022-12-13T06:46:06.777505Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def calc_acc(output, label):  \n    digits = int(output.shape[1] / 36)\n    output, label = output.view((-1, digits , 36)), label.view((-1, digits, 36))\n    output = nn.functional.softmax(output, dim=2)\n    #print(output.shape)\n    #print(label.shape)\n    output, label = torch.argmax(output, dim=2), torch.argmax(label, dim=2)\n    #print(label)\n    #print(output)\n    correct = torch.sum(output == label, dim=1)\n    #print(correct)\n    correct = torch.sum(correct == digits, dim=0)\n\n    return correct","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.780490Z","iopub.execute_input":"2022-12-13T06:46:06.781095Z","iopub.status.idle":"2022-12-13T06:46:06.791872Z","shell.execute_reply.started":"2022-12-13T06:46:06.781058Z","shell.execute_reply":"2022-12-13T06:46:06.790671Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train_data = []\nval_data = []\n\nwith open(f'{TRAIN_PATH}/annotations.csv', newline='') as csvfile:\n    for row in csv.reader(csvfile, delimiter=','):\n        if random.random() < 0.8:\n            train_data.append(row)\n        else:\n            val_data.append(row)\n\ntest_data = []\nwith open(f'{TEST_PATH}/../sample_submission.csv', newline='') as csvfile:\n    for row in csv.reader(csvfile, delimiter=','):\n        test_data.append(row)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.794340Z","iopub.execute_input":"2022-12-13T06:46:06.795096Z","iopub.status.idle":"2022-12-13T06:46:06.822582Z","shell.execute_reply.started":"2022-12-13T06:46:06.795059Z","shell.execute_reply":"2022-12-13T06:46:06.821754Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## TASK1","metadata":{}},{"cell_type":"code","source":"class Task1Dataset(Dataset):\n    def __init__(self, data, root, return_filename=False):\n        self.data = [sample for sample in data if sample[0].startswith(\"task1\")]\n        self.return_filename = return_filename\n        self.root = root\n    \n    def __getitem__(self, index):\n        filename, label = self.data[index]\n        img = read_image(f\"{self.root}/{filename}\")\n        img = torch.as_tensor(img, dtype=torch.float32)\n        transform = transforms.Compose([\n            transforms.Resize(size=224),\n            transforms.Normalize(mean=[0, 0, 0], std=[255, 255, 255]),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        #img = cv2.resize(img, (32, 32))\n        #img = np.mean(img, axis=2)\n        img = transform(img)\n        if self.return_filename:\n            return img, filename\n        else:\n            return img, int(label)\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.825215Z","iopub.execute_input":"2022-12-13T06:46:06.825848Z","iopub.status.idle":"2022-12-13T06:46:06.834721Z","shell.execute_reply.started":"2022-12-13T06:46:06.825811Z","shell.execute_reply":"2022-12-13T06:46:06.833719Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"train_ds = Task1Dataset(train_data, root=TRAIN_PATH)\ntrain_dl = DataLoader(train_ds, batch_size=100, num_workers=2, drop_last=True, shuffle=True)\n\nval_ds = Task1Dataset(val_data, root=TRAIN_PATH)\nval_dl = DataLoader(val_ds, batch_size=100, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.836599Z","iopub.execute_input":"2022-12-13T06:46:06.837084Z","iopub.status.idle":"2022-12-13T06:46:06.853202Z","shell.execute_reply.started":"2022-12-13T06:46:06.837045Z","shell.execute_reply":"2022-12-13T06:46:06.852356Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"task1_model = resnet18(pretrained=True)\ntask1_model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\ntask1_mode = task1_model.to(device)\n#print(model)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:06.921498Z","iopub.execute_input":"2022-12-13T06:46:06.921818Z","iopub.status.idle":"2022-12-13T06:46:07.157395Z","shell.execute_reply.started":"2022-12-13T06:46:06.921789Z","shell.execute_reply":"2022-12-13T06:46:07.156169Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"#model = Model().to(device)\noptimizer = torch.optim.Adam(task1_model.parameters(), lr=1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n\nfor epoch in range(15):\n    print(f\"Epoch [{epoch}]\")\n    task1_model.train()\n    for image, label in train_dl:\n        image = image.to(device)\n        label = label.to(device)\n        \n        pred = task1_model(image)\n        loss = loss_fn(pred, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        del image, label, pred\n        torch.cuda.empty_cache()\n        \n    sample_count = 0\n    correct_count = 0\n    task1_model.eval()\n    with torch.no_grad():\n        for image, label in val_dl:\n            image = image.to(device)\n            label = label.to(device)\n\n            pred = task1_model(image)\n            loss = loss_fn(pred, label)\n\n            pred = torch.argmax(pred, dim=1)\n\n            sample_count += len(image)\n            correct_count += (label == pred).sum()\n\n            del image, label, pred\n            torch.cuda.empty_cache()\n        \n    print(\"accuracy (validation):\", correct_count / sample_count)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:46:07.160097Z","iopub.execute_input":"2022-12-13T06:46:07.160552Z","iopub.status.idle":"2022-12-13T06:48:00.947143Z","shell.execute_reply.started":"2022-12-13T06:46:07.160507Z","shell.execute_reply":"2022-12-13T06:48:00.945119Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Epoch [0]\naccuracy (validation): tensor(0.9066, device='cuda:0')\nEpoch [1]\naccuracy (validation): tensor(0.9886, device='cuda:0')\nEpoch [2]\naccuracy (validation): tensor(0.9977, device='cuda:0')\nEpoch [3]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [4]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [5]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [6]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [7]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [8]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [9]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [10]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [11]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [12]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [13]\naccuracy (validation): tensor(1., device='cuda:0')\nEpoch [14]\naccuracy (validation): tensor(1., device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"test_ds = Task1Dataset(test_data, root=TEST_PATH, return_filename=True)\ntest_dl = DataLoader(test_ds, batch_size=100, drop_last=False, shuffle=False)\n\n\n\"\"\" if os.path.exists('submission.csv'):\n    csv_writer = csv.writer(open('submission.csv', 'a', newline=''))\nelse:\n\"\"\"\nif os.path.exists('submission.csv'):\n    os.remove('submission.csv')\n    \nfile = open('submission.csv', 'w', newline='')\ncsv_writer = csv.writer(file)\ncsv_writer.writerow([\"filename\", \"label\"])\n\n\ntask1_model.eval()\nwith torch.no_grad():\n    for image, filenames in test_dl:\n        image = image.to(device)\n\n        pred = task1_model(image)\n        pred = torch.argmax(pred, dim=1)\n\n        for i in range(len(filenames)):\n            csv_writer.writerow([filenames[i], str(pred[i].item())])\n\n        del image, pred\n        torch.cuda.empty_cache()\n    \nfile.close()\n\ndel task1_model, train_ds, train_dl, val_ds, val_dl, test_ds, test_dl\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:00.949756Z","iopub.execute_input":"2022-12-13T06:48:00.950916Z","iopub.status.idle":"2022-12-13T06:48:28.358719Z","shell.execute_reply.started":"2022-12-13T06:48:00.950868Z","shell.execute_reply":"2022-12-13T06:48:28.357706Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"## TASK2","metadata":{}},{"cell_type":"code","source":"class Task2Dataset(Dataset):\n    def __init__(self, data, root, return_filename=False):\n        self.data = [sample for sample in data if sample[0].startswith(\"task2\")]\n        self.return_filename = return_filename\n        self.root = root\n    \n    def __getitem__(self, index):\n        filename, label = self.data[index]\n        img = read_image(f\"{self.root}/{filename}\")\n        img = torch.as_tensor(img, dtype=torch.float32)\n        transform = transforms.Compose([\n            transforms.Resize(size=224),\n            transforms.Normalize(mean=[0, 0, 0], std=[255, 255, 255]),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        #img = cv2.resize(img, (32, 32))\n        #img = np.mean(img, axis=2)\n        img = transform(img)\n        if self.return_filename:\n            return img, filename\n        else:\n            #new_label = np.array([code[label[0]], code[label[1]]])\n            #new_label = [0] * 72\n            new_label = np.zeros(shape=72)\n            new_label[code[label[0]]] = 1\n            new_label[code[label[1]]+36] = 1\n            #new_label += np.array([0, 36])\n            new_label = torch.LongTensor(new_label)\n            return img, new_label\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:28.360439Z","iopub.execute_input":"2022-12-13T06:48:28.360842Z","iopub.status.idle":"2022-12-13T06:48:28.370887Z","shell.execute_reply.started":"2022-12-13T06:48:28.360803Z","shell.execute_reply":"2022-12-13T06:48:28.369825Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"train_ds = Task2Dataset(train_data, root=TRAIN_PATH)\ntrain_dl = DataLoader(train_ds, batch_size=100, num_workers=2, drop_last=True, shuffle=True)\n\nval_ds = Task2Dataset(val_data, root=TRAIN_PATH)\nval_dl = DataLoader(val_ds, batch_size=100, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:28.373974Z","iopub.execute_input":"2022-12-13T06:48:28.374962Z","iopub.status.idle":"2022-12-13T06:48:28.386669Z","shell.execute_reply.started":"2022-12-13T06:48:28.374933Z","shell.execute_reply":"2022-12-13T06:48:28.385660Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"class Task2Model(nn.Module):\n    def __init__(self):\n        super(Task2Model, self).__init__()\n\n        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        self.fc1 = nn.Linear(in_features=1000, out_features=36, bias=True)\n        self.fc2 = nn.Linear(in_features=1000, out_features=36, bias=True)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        output_1 = self.fc1(x)\n        output_2 = self.fc2(x)\n\n        return output_1, output_2\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:28.388162Z","iopub.execute_input":"2022-12-13T06:48:28.388597Z","iopub.status.idle":"2022-12-13T06:48:28.397935Z","shell.execute_reply.started":"2022-12-13T06:48:28.388563Z","shell.execute_reply":"2022-12-13T06:48:28.396993Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"task2_model = resnet18(pretrained=True)\ntask2_model.fc = nn.Linear(in_features=512, out_features=72, bias=True)\ntask2_model = task2_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:28.399198Z","iopub.execute_input":"2022-12-13T06:48:28.399706Z","iopub.status.idle":"2022-12-13T06:48:28.651437Z","shell.execute_reply.started":"2022-12-13T06:48:28.399665Z","shell.execute_reply":"2022-12-13T06:48:28.650331Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"#task2_model = Task2Model().to(device)\n#weights = ResNet18_Weights.IMAGENET1K_V1\n#preprocess = weights.transforms()\n\n#task2_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n#task2_model = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n#task2_model.fc = nn.Linear(in_features=512, out_features=72, bias=True)\n\noptimizer = torch.optim.Adam(task2_model.parameters(), lr=1e-3)\n#loss_fn = nn.CrossEntropyLoss()\nloss_fn = nn.MultiLabelSoftMarginLoss()\n\n\nfor epoch in range(15):\n    print(f\"Epoch [{epoch}]\")\n    task2_model.train()\n\n    train_loss = 0.0\n    for image, label in train_dl:\n\n        image = image.to(device)\n        #label_1, label_2 = label[:, 0], label[:, 1]\n        label = label.to(device)\n        #label_1, label_2 = label_1.to(device, dtype=torch.long), label_2.to(device, dtype=torch.long)\n        \n        #pred_1, pred_2 = task2_model(image)\n        #image_transformed = preprocess(image)\n\n        pred = task2_model(image)\n        loss = loss_fn(pred, label)\n        #loss_1, loss_2 = loss_fn(pred_1, label_1), loss_fn(pred_2, label_2)\n        \n        #loss = loss_1 + loss_2\n        train_loss += loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        del image, label, pred\n        torch.cuda.empty_cache()\n    \n    #print(len(train_dl))\n    train_loss /= len(train_dl)\n    print(\"train loss: {}\".format(train_loss))\n\n    sample_count = 0\n    correct_count = 0\n    task2_model.eval()\n    val_loss = 0\n    \n    with torch.no_grad():\n        for image, label in val_dl:\n            image = image.to(device)\n            label = label.to(device)\n            #label_1, label_2 = label[:, 0], label[:, 1]\n            #label_1, label_2 = label_1.to(device, dtype=torch.long), label_2.to(device, dtype=torch.long)\n\n            #pred_1, pred_2 = task2_model(image)\n            #loss_1, loss_2 = loss_fn(pred_1, label_1), loss_fn(pred_2, label_2)\n\n            #loss = loss_1 + loss_2\n            #val_loss += loss\n            #print(\"loss: {}\".format(loss))\n\n            #image_transformed = preprocess(image)\n\n            pred = task2_model(image)\n            loss = loss_fn(pred, label)\n            #print(\"pred: \", pred)\n            #print(\"label: \", label)\n            val_loss += loss\n            #pred_1, pred_2 = torch.argmax(pred_1, dim=1), torch.argmax(pred_2, dim=1)\n            #pred = torch.stack([pred_1, pred_2], dim=1)\n\n            sample_count += len(image)\n            #correct = torch.sum(pred == label, dim=1)\n            #print(correct)\n            correct_count += calc_acc(pred, label)\n            #correct_count += torch.sum(correct == 2)\n\n            del image, label, pred\n            torch.cuda.empty_cache()\n\n        val_loss /= len(val_dl)\n        print(\"val loss: {}\".format(val_loss))\n        \n    print(\"accuracy (validation):\", correct_count / sample_count)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:48:28.652919Z","iopub.execute_input":"2022-12-13T06:48:28.653340Z","iopub.status.idle":"2022-12-13T06:50:49.142378Z","shell.execute_reply.started":"2022-12-13T06:48:28.653280Z","shell.execute_reply":"2022-12-13T06:50:49.141058Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Epoch [0]\ntrain loss: 0.19348277151584625\nval loss: 0.10719960927963257\naccuracy (validation): tensor(0.0519, device='cuda:0')\nEpoch [1]\ntrain loss: 0.06034605950117111\nval loss: 0.04555454105138779\naccuracy (validation): tensor(0.7545, device='cuda:0')\nEpoch [2]\ntrain loss: 0.0251605361700058\nval loss: 0.021599553525447845\naccuracy (validation): tensor(0.9521, device='cuda:0')\nEpoch [3]\ntrain loss: 0.011988703161478043\nval loss: 0.013204094022512436\naccuracy (validation): tensor(0.9860, device='cuda:0')\nEpoch [4]\ntrain loss: 0.007252005394548178\nval loss: 0.010282653383910656\naccuracy (validation): tensor(0.9840, device='cuda:0')\nEpoch [5]\ntrain loss: 0.005042686592787504\nval loss: 0.00767285143956542\naccuracy (validation): tensor(0.9860, device='cuda:0')\nEpoch [6]\ntrain loss: 0.0037929932586848736\nval loss: 0.006092037074267864\naccuracy (validation): tensor(0.9880, device='cuda:0')\nEpoch [7]\ntrain loss: 0.003057674737647176\nval loss: 0.0048919557593762875\naccuracy (validation): tensor(0.9940, device='cuda:0')\nEpoch [8]\ntrain loss: 0.002549532800912857\nval loss: 0.003919989801943302\naccuracy (validation): tensor(0.9920, device='cuda:0')\nEpoch [9]\ntrain loss: 0.0021514054387807846\nval loss: 0.0035030613653361797\naccuracy (validation): tensor(0.9920, device='cuda:0')\nEpoch [10]\ntrain loss: 0.001866648904979229\nval loss: 0.0033456278033554554\naccuracy (validation): tensor(0.9900, device='cuda:0')\nEpoch [11]\ntrain loss: 0.0016663341084495187\nval loss: 0.0029501791577786207\naccuracy (validation): tensor(0.9960, device='cuda:0')\nEpoch [12]\ntrain loss: 0.0014581283321604133\nval loss: 0.002713743131607771\naccuracy (validation): tensor(0.9900, device='cuda:0')\nEpoch [13]\ntrain loss: 0.001337798428721726\nval loss: 0.002558355452492833\naccuracy (validation): tensor(0.9900, device='cuda:0')\nEpoch [14]\ntrain loss: 0.001229569548740983\nval loss: 0.002057092497125268\naccuracy (validation): tensor(0.9940, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"test_ds = Task2Dataset(test_data, root=TEST_PATH, return_filename=True)\ntest_dl = DataLoader(test_ds, batch_size=100, drop_last=False, shuffle=False)\n\nfile = open('submission.csv', 'a', newline='')\ncsv_writer = csv.writer(file)\n\ntask2_model.eval()\nwith torch.no_grad():\n    for image, filenames in test_dl:\n        image = image.to(device)\n\n        pred = task2_model(image)\n        pred = pred.view(-1, 2, 36)\n        pred = torch.argmax(pred, dim=2)\n\n        for i in range(len(filenames)):\n            csv_writer.writerow([filenames[i], rev_code[pred[i][0].item()] + rev_code[pred[i][1].item()]])\n\n        del image, pred\n        torch.cuda.empty_cache()\n    \nfile.close()\ndel task2_model, train_ds, train_dl, val_ds, val_dl, test_ds, test_dl\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:50:49.144734Z","iopub.execute_input":"2022-12-13T06:50:49.145613Z","iopub.status.idle":"2022-12-13T06:51:00.588478Z","shell.execute_reply.started":"2022-12-13T06:50:49.145568Z","shell.execute_reply":"2022-12-13T06:51:00.587405Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"## TASK3","metadata":{}},{"cell_type":"code","source":"class Task3Dataset(Dataset):\n    def __init__(self, data, root, return_filename=False):\n        self.data = [sample for sample in data if sample[0].startswith(\"task3\")]\n        self.return_filename = return_filename\n        self.root = root\n    \n    def __getitem__(self, index):\n        filename, label = self.data[index]\n        img = read_image(f\"{self.root}/{filename}\")\n        img = torch.as_tensor(img, dtype=torch.float32)\n        transform = transforms.Compose([\n            transforms.Resize(size=224),\n            transforms.Normalize(mean=[0, 0, 0], std=[255, 255, 255]),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n        #img = cv2.resize(img, (32, 32))\n        #img = np.mean(img, axis=2)\n        img = transform(img)\n        if self.return_filename:\n            return img, filename\n        else:\n            new_label = np.zeros(shape=144)\n            for i in range(4):\n                new_label[code[label[i]]+i*36] = 1\n            new_label = torch.LongTensor(new_label)\n            return img, new_label\n\n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:51:00.590164Z","iopub.execute_input":"2022-12-13T06:51:00.590561Z","iopub.status.idle":"2022-12-13T06:51:00.601122Z","shell.execute_reply.started":"2022-12-13T06:51:00.590525Z","shell.execute_reply":"2022-12-13T06:51:00.599997Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"train_ds = Task3Dataset(train_data, root=TRAIN_PATH)\ntrain_dl = DataLoader(train_ds, batch_size=100, num_workers=2, drop_last=True, shuffle=True)\n\nval_ds = Task3Dataset(val_data, root=TRAIN_PATH)\nval_dl = DataLoader(val_ds, batch_size=100, num_workers=2, drop_last=False, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:51:00.602714Z","iopub.execute_input":"2022-12-13T06:51:00.603391Z","iopub.status.idle":"2022-12-13T06:51:00.614265Z","shell.execute_reply.started":"2022-12-13T06:51:00.603270Z","shell.execute_reply":"2022-12-13T06:51:00.613048Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class Task3Model(nn.Module):\n    def __init__(self):\n        super(Task3Model, self).__init__()\n\n        self.resnet = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n        self.fc1 = nn.Linear(in_features=1000, out_features=36, bias=True)\n        self.fc2 = nn.Linear(in_features=1000, out_features=36, bias=True)\n        self.fc3 = nn.Linear(in_features=1000, out_features=36, bias=True)\n        self.fc4 = nn.Linear(in_features=1000, out_features=36, bias=True)\n\n    def forward(self, x):\n        x = self.resnet(x)\n        output_1 = self.fc1(x)\n        output_2 = self.fc2(x)\n        output_3 = self.fc3(x)\n        output_4 = self.fc4(x)\n\n        return output_1, output_2, output_3, output_4","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:51:00.615562Z","iopub.execute_input":"2022-12-13T06:51:00.615996Z","iopub.status.idle":"2022-12-13T06:51:00.628498Z","shell.execute_reply.started":"2022-12-13T06:51:00.615962Z","shell.execute_reply":"2022-12-13T06:51:00.627556Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"task3_model = resnet18(pretrained=True)\ntask3_model.fc = nn.Linear(in_features=512, out_features=144, bias=True)\ntask3_model = task3_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:51:00.629815Z","iopub.execute_input":"2022-12-13T06:51:00.630344Z","iopub.status.idle":"2022-12-13T06:51:00.868659Z","shell.execute_reply.started":"2022-12-13T06:51:00.630292Z","shell.execute_reply":"2022-12-13T06:51:00.867494Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#task3_model = Task3Model().to(device)\n\noptimizer = torch.optim.Adam(task3_model.parameters(), lr=1e-3)\nloss_fn = nn.MultiLabelSoftMarginLoss()\n#loss_fn = nn.CrossEntropyLoss()\n\n\nfor epoch in range(50):\n    print(f\"Epoch [{epoch}]\")\n    task3_model.train()\n\n    train_loss = 0.0\n    for image, label in train_dl:\n\n        image = image.to(device)\n        #label_1, label_2 = label[:, 0], label[:, 1]\n        label = label.to(device)\n        #label_1, label_2 = label_1.to(device, dtype=torch.long), label_2.to(device, dtype=torch.long)\n        \n        #pred_1, pred_2 = task2_model(image)\n        #image_transformed = preprocess(image)\n\n        pred = task3_model(image)\n        loss = loss_fn(pred, label)\n        #loss_1, loss_2 = loss_fn(pred_1, label_1), loss_fn(pred_2, label_2)\n        \n        #loss = loss_1 + loss_2\n        train_loss += loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        del image, label, pred\n        torch.cuda.empty_cache()\n    \n    #print(len(train_dl))\n    train_loss /= len(train_dl)\n    print(\"train loss: {}\".format(train_loss))\n\n    sample_count = 0\n    correct_count = 0\n    task3_model.eval()\n    val_loss = 0\n    \n    with torch.no_grad():\n        for image, label in val_dl:\n            image = image.to(device)\n            label = label.to(device)\n            #label_1, label_2 = label[:, 0], label[:, 1]\n            #label_1, label_2 = label_1.to(device, dtype=torch.long), label_2.to(device, dtype=torch.long)\n\n            #pred_1, pred_2 = task2_model(image)\n            #loss_1, loss_2 = loss_fn(pred_1, label_1), loss_fn(pred_2, label_2)\n\n            #loss = loss_1 + loss_2\n            #val_loss += loss\n            #print(\"loss: {}\".format(loss))\n\n            #image_transformed = preprocess(image)\n\n            pred = task3_model(image)\n            loss = loss_fn(pred, label)\n            #print(\"pred: \", pred)\n            #print(\"label: \", label)\n            val_loss += loss\n            #pred_1, pred_2 = torch.argmax(pred_1, dim=1), torch.argmax(pred_2, dim=1)\n            #pred = torch.stack([pred_1, pred_2], dim=1)\n\n            sample_count += len(image)\n            #correct = torch.sum(pred == label, dim=1)\n            #print(correct)\n            correct_count += calc_acc(pred, label)\n            #correct_count += torch.sum(correct == 2)\n\n            del image, label, pred\n            torch.cuda.empty_cache()\n\n        val_loss /= len(val_dl)\n        print(\"val loss: {}\".format(val_loss))\n        \n    print(\"accuracy (validation):\", correct_count / sample_count)","metadata":{"execution":{"iopub.status.busy":"2022-12-13T06:51:00.872580Z","iopub.execute_input":"2022-12-13T06:51:00.873084Z","iopub.status.idle":"2022-12-13T07:03:44.411205Z","shell.execute_reply.started":"2022-12-13T06:51:00.873048Z","shell.execute_reply":"2022-12-13T07:03:44.410057Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Epoch [0]\ntrain loss: 0.19428998231887817\nval loss: 0.17421244084835052\naccuracy (validation): tensor(0., device='cuda:0')\nEpoch [1]\ntrain loss: 0.11824437230825424\nval loss: 0.11881878226995468\naccuracy (validation): tensor(0., device='cuda:0')\nEpoch [2]\ntrain loss: 0.10297312587499619\nval loss: 0.10125190019607544\naccuracy (validation): tensor(0., device='cuda:0')\nEpoch [3]\ntrain loss: 0.0792708545923233\nval loss: 0.07412423938512802\naccuracy (validation): tensor(0.0769, device='cuda:0')\nEpoch [4]\ntrain loss: 0.055982254445552826\nval loss: 0.051472704857587814\naccuracy (validation): tensor(0.3658, device='cuda:0')\nEpoch [5]\ntrain loss: 0.037778645753860474\nval loss: 0.03852808475494385\naccuracy (validation): tensor(0.7060, device='cuda:0')\nEpoch [6]\ntrain loss: 0.02555079013109207\nval loss: 0.028421461582183838\naccuracy (validation): tensor(0.8188, device='cuda:0')\nEpoch [7]\ntrain loss: 0.017628397792577744\nval loss: 0.02138073556125164\naccuracy (validation): tensor(0.9077, device='cuda:0')\nEpoch [8]\ntrain loss: 0.01278240792453289\nval loss: 0.018044166266918182\naccuracy (validation): tensor(0.9333, device='cuda:0')\nEpoch [9]\ntrain loss: 0.009833513759076595\nval loss: 0.016607483848929405\naccuracy (validation): tensor(0.9333, device='cuda:0')\nEpoch [10]\ntrain loss: 0.007727352436631918\nval loss: 0.013011371716856956\naccuracy (validation): tensor(0.9521, device='cuda:0')\nEpoch [11]\ntrain loss: 0.0062894499860703945\nval loss: 0.01057556550949812\naccuracy (validation): tensor(0.9538, device='cuda:0')\nEpoch [12]\ntrain loss: 0.005140996538102627\nval loss: 0.009854099713265896\naccuracy (validation): tensor(0.9538, device='cuda:0')\nEpoch [13]\ntrain loss: 0.004320712294429541\nval loss: 0.00832432508468628\naccuracy (validation): tensor(0.9692, device='cuda:0')\nEpoch [14]\ntrain loss: 0.003600743366405368\nval loss: 0.007638325449079275\naccuracy (validation): tensor(0.9709, device='cuda:0')\nEpoch [15]\ntrain loss: 0.0030533417593687773\nval loss: 0.006762627512216568\naccuracy (validation): tensor(0.9692, device='cuda:0')\nEpoch [16]\ntrain loss: 0.0026196041144430637\nval loss: 0.006274496670812368\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [17]\ntrain loss: 0.00228051096200943\nval loss: 0.005871946923434734\naccuracy (validation): tensor(0.9709, device='cuda:0')\nEpoch [18]\ntrain loss: 0.002081030048429966\nval loss: 0.005310317035764456\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [19]\ntrain loss: 0.0018570004031062126\nval loss: 0.005059665068984032\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [20]\ntrain loss: 0.0016745894681662321\nval loss: 0.0047076791524887085\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [21]\ntrain loss: 0.0014751437120139599\nval loss: 0.004792501218616962\naccuracy (validation): tensor(0.9761, device='cuda:0')\nEpoch [22]\ntrain loss: 0.0013309428468346596\nval loss: 0.004435169510543346\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [23]\ntrain loss: 0.0012267265701666474\nval loss: 0.004180338233709335\naccuracy (validation): tensor(0.9744, device='cuda:0')\nEpoch [24]\ntrain loss: 0.0011208245996385813\nval loss: 0.004125379025936127\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [25]\ntrain loss: 0.001052018953487277\nval loss: 0.0040311748161911964\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [26]\ntrain loss: 0.0009838314726948738\nval loss: 0.0038408495020121336\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [27]\ntrain loss: 0.0008934563375078142\nval loss: 0.0035595078952610493\naccuracy (validation): tensor(0.9761, device='cuda:0')\nEpoch [28]\ntrain loss: 0.0008204075857065618\nval loss: 0.0035729380324482918\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [29]\ntrain loss: 0.0007609119638800621\nval loss: 0.0033521600998938084\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [30]\ntrain loss: 0.0007290594512596726\nval loss: 0.003449959447607398\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [31]\ntrain loss: 0.0006953255506232381\nval loss: 0.003263683058321476\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [32]\ntrain loss: 0.0006313882768154144\nval loss: 0.0031370611395686865\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [33]\ntrain loss: 0.0005942200077697635\nval loss: 0.0030756047926843166\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [34]\ntrain loss: 0.0005544790765270591\nval loss: 0.0029134328942745924\naccuracy (validation): tensor(0.9829, device='cuda:0')\nEpoch [35]\ntrain loss: 0.0005246971268206835\nval loss: 0.0029796864837408066\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [36]\ntrain loss: 0.0004943990497849882\nval loss: 0.0028775716200470924\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [37]\ntrain loss: 0.0004749493673443794\nval loss: 0.002825687639415264\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [38]\ntrain loss: 0.00046248786384239793\nval loss: 0.0027923358138650656\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [39]\ntrain loss: 0.00042593860416673124\nval loss: 0.0026790981646627188\naccuracy (validation): tensor(0.9778, device='cuda:0')\nEpoch [40]\ntrain loss: 0.0004058073682244867\nval loss: 0.0026983132120221853\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [41]\ntrain loss: 0.000403343205107376\nval loss: 0.0026436499319970608\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [42]\ntrain loss: 0.0003746644069906324\nval loss: 0.0025415769778192043\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [43]\ntrain loss: 0.00035463759559206665\nval loss: 0.002496254164725542\naccuracy (validation): tensor(0.9795, device='cuda:0')\nEpoch [44]\ntrain loss: 0.00033725681714713573\nval loss: 0.0025470166001468897\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [45]\ntrain loss: 0.0003239052020944655\nval loss: 0.002470503794029355\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [46]\ntrain loss: 0.00030715600587427616\nval loss: 0.002465074183419347\naccuracy (validation): tensor(0.9761, device='cuda:0')\nEpoch [47]\ntrain loss: 0.0002916351077146828\nval loss: 0.0024258459452539682\naccuracy (validation): tensor(0.9761, device='cuda:0')\nEpoch [48]\ntrain loss: 0.0002776899200398475\nval loss: 0.002379970159381628\naccuracy (validation): tensor(0.9812, device='cuda:0')\nEpoch [49]\ntrain loss: 0.00026972463820129633\nval loss: 0.002387744840234518\naccuracy (validation): tensor(0.9778, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"test_ds = Task3Dataset(test_data, root=TEST_PATH, return_filename=True)\ntest_dl = DataLoader(test_ds, batch_size=100, drop_last=False, shuffle=False)\n\nfile = open('submission.csv', 'a', newline='')\ncsv_writer = csv.writer(file)\n\ntask3_model.eval()\n\nwith torch.no_grad():\n    for image, filenames in test_dl:\n        image = image.to(device)\n\n        pred = task3_model(image)\n        pred = pred.view(-1, 4, 36)\n        pred = torch.argmax(pred, dim=2)\n\n        for i in range(len(filenames)):\n            csv_writer.writerow([filenames[i], rev_code[pred[i][0].item()] + rev_code[pred[i][1].item()] + \\\n                                                rev_code[pred[i][2].item()] + rev_code[pred[i][3].item()]])    \n\n        del image, pred\n        torch.cuda.empty_cache()\n    \nfile.close()    \ndel task3_model, train_ds, train_dl, val_ds, val_dl, test_ds, test_dl\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-12-13T07:03:44.413294Z","iopub.execute_input":"2022-12-13T07:03:44.413762Z","iopub.status.idle":"2022-12-13T07:03:50.425890Z","shell.execute_reply.started":"2022-12-13T07:03:44.413719Z","shell.execute_reply":"2022-12-13T07:03:50.424787Z"},"trusted":true},"execution_count":69,"outputs":[]}]}