{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./input\\sample_submission.csv\n",
      "./input\\test.csv\n",
      "./input\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "INPUT_DIR = './input'\n",
    "#INPUT_DIR = '/kaggle/input'\n",
    "TRAIN_PATH = INPUT_DIR + '/train.csv'\n",
    "TEST_PATH = INPUT_DIR + '/test.csv'\n",
    "SUBMISSION_PATH = INPUT_DIR + '/sample_submission.csv'\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(INPUT_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def dfToNumpy(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    num_of_rows = len(df['id'])\n",
    "\n",
    "    df = df.drop(['id', 'product_code', 'failure'], axis=1)   \n",
    "    df['area'] = df['attribute_2'] * df['attribute_3']\n",
    "    df = df.drop(['attribute_2', 'attribute_3'], axis=1)\n",
    "\n",
    "    num_col = 0\n",
    "\n",
    "    df['attribute_1'] = df['attribute_1'].map({'material_5': 5, 'material_6': 6, 'material_7': 7, 'material_8': 8})\n",
    "    df['attribute_0'] = df['attribute_0'].map({'material_5': 5, 'material_6': 6, 'material_7': 7, 'material_8': 8})\n",
    "    \n",
    "    print(\"Data Frame has columns: \", df.columns.to_list())\n",
    "    features = df.to_numpy()\n",
    "    \n",
    "    return features\n",
    "            \n",
    "\n",
    "def getLabel(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    label = df['failure'].to_numpy()\n",
    "    print(\"label shape is: \", label.shape)\n",
    "\n",
    "    return label\n",
    "\n",
    "def handleMissingValue(features):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    new_features = imputer.fit_transform(features)\n",
    "\n",
    "    return imputer, new_features\n",
    "\n",
    "def Normalize(features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features)\n",
    "    print(\"Get mean of each column: \", scaler.mean_)\n",
    "    print(\"Get std of each column: \", np.sqrt(scaler.var_))\n",
    "\n",
    "    new_features = scaler.transform(features)\n",
    "    \n",
    "    return scaler, new_features\n",
    "\n",
    "\n",
    "#dfToNumpy(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame has columns:  ['loading', 'attribute_0', 'attribute_1', 'measurement_0', 'measurement_1', 'measurement_2', 'measurement_3', 'measurement_4', 'measurement_5', 'measurement_6', 'measurement_7', 'measurement_8', 'measurement_9', 'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13', 'measurement_14', 'measurement_15', 'measurement_16', 'measurement_17', 'area']\n",
      "label shape is:  (26570,)\n",
      "Get mean of each column:  [127.82518954   6.60481746   6.42785096   7.41588257   8.23251788\n",
      "   6.25656756  17.79081431  11.73289076  17.12790676  17.51052087\n",
      "  11.71878737  19.02333165  11.4294161   16.12174455  19.17448997\n",
      "  11.70316005  15.64894519  16.04278097  14.99821438  16.4589192\n",
      " 701.45757606  47.74761009]\n",
      "Get std of each column:  [ 38.88037008   0.79636414   1.3574283    4.11661265   4.19932206\n",
      "   3.30904702   0.99544177   0.98791527   0.98595938   0.9838503\n",
      "   0.98642304   0.99251951   0.98049454   1.37971252   1.48789755\n",
      "   1.45336496   1.12430125   1.45015767   1.50283064   1.65280305\n",
      " 118.99307502  10.05375741]\n",
      "New Features:  [[-1.22748805  0.49623347  1.15818201 ... -1.07388427  0.52643756\n",
      "  -0.27329186]\n",
      " [-1.10428963  0.49623347  1.15818201 ... -0.50091824 -0.16303954\n",
      "  -0.27329186]\n",
      " [-1.16756063  0.49623347  1.15818201 ...  0.89973261 -0.32003187\n",
      "  -0.27329186]\n",
      " ...\n",
      " [-0.3139165   0.49623347 -0.31519231 ... -1.44839955  0.41100227\n",
      "   0.62189584]\n",
      " [-0.55156855  0.49623347 -0.31519231 ... -2.17443887  0.24117726\n",
      "   0.62189584]\n",
      " [ 0.08679985  0.49623347 -0.31519231 ...  0.86585077 -0.83285163\n",
      "   0.62189584]]\n"
     ]
    }
   ],
   "source": [
    "features = dfToNumpy(TRAIN_PATH)\n",
    "labels = getLabel(TRAIN_PATH)\n",
    "imputer, features = handleMissingValue(features)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Get new features and mean and std of each column\n",
    "'''\n",
    "scaler, features = Normalize(features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, labels, test_size=0.1)\n",
    "\n",
    "#print(\"New Features: \", features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA: For Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reduction, the dimension of features become:  (26570, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.8)\n",
    "x_train = pca.fit_transform(x_train)\n",
    "\n",
    "print(\"After reduction, the dimension of features become: \", x_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model for classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "features, _ , y = shuffle(features, labels)\n",
    "param = {\"C\": stats.uniform(1, 10),\n",
    "             \"gamma\": stats.uniform(0.01, 1)}\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "clf = RandomizedSearchCV(svc, param_distributions=param, n_iter=20, cv=5, scoring='auc')\n",
    "model = clf.fit(features, labels)\n",
    "print(\"best parameters: \", model.best_params_)\n",
    "print(\"Search Result\")\n",
    "print(model.cv_results_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c6da53ade2302c5c2549050a755f7271f56c4989f0bdbb18e4ce6e64f091b09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
