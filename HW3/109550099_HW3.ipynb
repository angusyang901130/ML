{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your implementations right here to check your result\n",
    "# (Of course you can add your classes not written here)\n",
    "def gini(sequence):\n",
    "\n",
    "    cls_count = {}\n",
    "    for c in sequence:\n",
    "        if c in cls_count.keys():\n",
    "            cls_count[c] += 1\n",
    "        else:\n",
    "            cls_count[c] = 1\n",
    "    \n",
    "    Gini = 1.0\n",
    "    for c in cls_count.keys():\n",
    "        p = cls_count[c] / len(sequence)\n",
    "        Gini -= p ** 2\n",
    "\n",
    "    return Gini\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    cls_count = {}\n",
    "    for c in sequence:\n",
    "        if c in cls_count.keys():\n",
    "            cls_count[c] += 1\n",
    "        else:\n",
    "            cls_count[c] = 1\n",
    "\n",
    "    Entropy = 0.0\n",
    "    for c in cls_count.keys():\n",
    "        p = cls_count[c] / len(sequence)\n",
    "        Entropy -= p * math.log2(p)\n",
    "    \n",
    "    return Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006402\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "val_df = pd.read_csv('val.csv')\n",
    "\n",
    "x_train = train_df.drop('price_range', axis=1)\n",
    "x_test = val_df.drop('price_range', axis=1)\n",
    "\n",
    "feature_names = x_train.columns.values\n",
    "\n",
    "x_train = x_train.values\n",
    "y_train = train_df['price_range'].values\n",
    "\n",
    "x_test = x_test.values\n",
    "y_test = val_df['price_range'].values\n",
    "\n",
    "#print(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, left_cls=None, right_cls=None, feature=None, threshold=None, left=None, right=None, is_leaf=True, depth=0):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "        self.depth = depth\n",
    "        self.left_cls = left_cls\n",
    "        self.right_cls = right_cls\n",
    "    \n",
    "    def fillin(self, feature, threshold, is_leaf, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.is_leaf = is_leaf\n",
    "        self.left = left\n",
    "        self.right = right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node()\n",
    "\n",
    "    def fit(self, x_data, y_data, node=None, indices=None, max_features=None):\n",
    "        \n",
    "        # indices -> the indices of data that the sample picks \n",
    "        # features -> the indices of feature that the node picks\n",
    "        \n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        if indices is None:\n",
    "            indices = np.array([i for i in range(x_data.shape[0])])\n",
    "\n",
    "        if max_features == x_data.shape[1] or max_features is None:\n",
    "            features = np.array([i for i in range(x_data.shape[1])])\n",
    "        else:\n",
    "            features = np.random.choice(x_data.shape[1], max_features, replace=False)\n",
    "\n",
    "        y_part = [y_data[idx] for idx in indices]\n",
    "        pure = self.check_pure(y_part)\n",
    "\n",
    "        if pure == 0:\n",
    "            node.left_cls, node.right_cls = 0, 0\n",
    "            return\n",
    "        elif pure == 1:\n",
    "            node.left_cls, node.right_cls = 1, 1\n",
    "            return\n",
    "        \n",
    "        # find feature and threshold\n",
    "        feature, threshold = self.find_threshold(x_data, y_data, indices, features)\n",
    "\n",
    "        # fill in information of node and split data\n",
    "        is_leaf = node.depth == self.max_depth-1 or feature == -1\n",
    "        left_node, right_node = None, None\n",
    "        \n",
    "        if not is_leaf:\n",
    "            left_node = Node(depth=node.depth+1)\n",
    "            right_node = Node(depth=node.depth+1)\n",
    "        \n",
    "        \n",
    "        left_indices, right_indices = self.split(x_data, feature, threshold, indices)\n",
    "        node.fillin(feature, threshold, is_leaf, left_node, right_node)\n",
    "\n",
    "        # if leaf -> check the class of right and left\n",
    "        if not is_leaf:\n",
    "            self.fit(x_data, y_data, node=left_node, indices=left_indices, max_features=max_features)\n",
    "            self.fit(x_data, y_data, node=right_node, indices=right_indices, max_features=max_features)\n",
    "        else:\n",
    "\n",
    "            left_y = np.array([y_data[idx] for idx in left_indices])\n",
    "            right_y = None\n",
    "\n",
    "            if right_indices is not None:\n",
    "                right_y = np.array([y_data[idx] for idx in right_indices])\n",
    "                \n",
    "            if self.check_class(left_y):\n",
    "                node.left_cls = 1\n",
    "            else:\n",
    "                node.left_cls = 0\n",
    "            \n",
    "            if right_y is not None and self.check_class(right_y):\n",
    "                node.right_cls = 1\n",
    "            else:\n",
    "                node.right_cls = 0\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        y_pred = np.zeros(shape=x_data.shape[0])\n",
    "\n",
    "        for i in range(x_data.shape[0]):\n",
    "            y_pred[i] = self.traverse(x_data[i], self.root)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    ###################################################\n",
    "    # check what class is for the leaf node\n",
    "    ###################################################\n",
    "    def check_class(self, y_data):\n",
    "        count = 0\n",
    "        for c in y_data:\n",
    "            if c == 0:\n",
    "                count += 1\n",
    "\n",
    "        if count > y_data.shape[0] - count:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    ##################################################\n",
    "    # check purity\n",
    "    ##################################################\n",
    "\n",
    "    def check_pure(self, y_data):\n",
    "        count = 0\n",
    "        y_data = np.array(y_data)\n",
    "        for c in y_data:\n",
    "            if c == 0:\n",
    "                count += 1\n",
    "\n",
    "        if count == y_data.shape[0]:\n",
    "            return 0\n",
    "        elif count == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    #################################################\n",
    "    # find_threshold\n",
    "    #################################################\n",
    "    def find_threshold(self, x_data, y_data, indices, features):\n",
    "        min_crit = 10.0\n",
    "        feature = -1\n",
    "        threshold = 0\n",
    "        \n",
    "        # run all features and unique values to find the threshold and feature\n",
    "        for col in features:\n",
    "            values = x_data[:, col] \n",
    "            values = np.unique(values)\n",
    "\n",
    "            if len(values) == 1:\n",
    "                continue\n",
    "\n",
    "            values = values[:-1]\n",
    "            \n",
    "            for value in values:\n",
    "                \n",
    "                left_indices = np.array([idx for idx in indices if x_data[idx][col] <= value])\n",
    "                right_indices = np.array([idx for idx in indices if x_data[idx][col] > value])\n",
    "\n",
    "                left = np.array([y_data[idx] for idx in left_indices])\n",
    "                right = np.array([y_data[idx] for idx in right_indices])\n",
    "\n",
    "                total = len(left_indices) + len(right_indices)\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    crit_value = len(left_indices) / total * gini(left) + len(right_indices) / total * gini(right)\n",
    "                else:\n",
    "                    crit_value = len(left_indices) / total * entropy(left) + len(right_indices) / total * entropy(right)\n",
    "\n",
    "                if crit_value < min_crit:\n",
    "                    min_crit = crit_value\n",
    "                    feature = col\n",
    "                    threshold = value\n",
    "\n",
    "        #print(min_crit)\n",
    "        #print(\"threshold: {th}  feature: {f}\".format(th=threshold, f=feature))\n",
    "        #print(feature, threshold)    \n",
    "        return (feature, threshold)\n",
    "\n",
    "    ###############################################\n",
    "    # split dataset\n",
    "    ###############################################\n",
    "    def split(self, x_data, feature, threshold, indices):\n",
    "        if feature == -1:\n",
    "            return indices, None\n",
    "\n",
    "        left_indices = np.array([idx for idx in indices if x_data[idx][feature] <= threshold])\n",
    "        right_indices = np.array([idx for idx in indices if x_data[idx][feature] > threshold])\n",
    "\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    ###############################################\n",
    "    # traverse during predict\n",
    "    ###############################################\n",
    "    def traverse(self, x, node):\n",
    "        if node.feature is None or node.feature == -1:\n",
    "            return node.left_cls\n",
    "        elif node.is_leaf:\n",
    "            if x[node.feature] <= node.threshold:\n",
    "                return node.left_cls\n",
    "            elif x[node.feature] > node.threshold:\n",
    "                return node.right_cls\n",
    "\n",
    "        if node.left is not None and x[node.feature] <= node.threshold:\n",
    "            return self.traverse(x, node.left)\n",
    "        elif node.right is not None and x[node.feature] > node.threshold:\n",
    "            return self.traverse(x, node.right)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_data, y_pred):\n",
    "    correct = 0\n",
    "    \n",
    "    for i in range(len(y_data)):\n",
    "        if y_data[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    acc = correct / len(y_data)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m clf_depth3 \u001b[39m=\u001b[39m DecisionTree(criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m      2\u001b[0m clf_depth10 \u001b[39m=\u001b[39m DecisionTree(criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgini\u001b[39m\u001b[39m'\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m clf_depth3\u001b[39m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m      5\u001b[0m clf_depth10\u001b[39m.\u001b[39mfit(x_train, y_train)\n",
      "Cell \u001b[1;32mIn [8], line 34\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, x_data, y_data, node, indices, max_features)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# find feature and threshold\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m feature, threshold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_threshold(x_data, y_data, indices, features)\n\u001b[0;32m     36\u001b[0m \u001b[39m# fill in information of node and split data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m is_leaf \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mdepth \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_depth\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m feature \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "Cell \u001b[1;32mIn [8], line 133\u001b[0m, in \u001b[0;36mDecisionTree.find_threshold\u001b[1;34m(self, x_data, y_data, indices, features)\u001b[0m\n\u001b[0;32m    130\u001b[0m left_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices \u001b[39mif\u001b[39;00m x_data[idx][col] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m value])\n\u001b[0;32m    131\u001b[0m right_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices \u001b[39mif\u001b[39;00m x_data[idx][col] \u001b[39m>\u001b[39m value])\n\u001b[1;32m--> 133\u001b[0m left \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([y_data[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m left_indices])\n\u001b[0;32m    134\u001b[0m right \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([y_data[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m right_indices])\n\u001b[0;32m    136\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(left_indices) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(right_indices)\n",
      "Cell \u001b[1;32mIn [8], line 133\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    130\u001b[0m left_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices \u001b[39mif\u001b[39;00m x_data[idx][col] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m value])\n\u001b[0;32m    131\u001b[0m right_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([idx \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices \u001b[39mif\u001b[39;00m x_data[idx][col] \u001b[39m>\u001b[39m value])\n\u001b[1;32m--> 133\u001b[0m left \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([y_data[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m left_indices])\n\u001b[0;32m    134\u001b[0m right \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([y_data[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m right_indices])\n\u001b[0;32m    136\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(left_indices) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(right_indices)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "\n",
    "clf_depth3.fit(x_train, y_train)\n",
    "clf_depth10.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with depth 3: 0.9166666666666666\n",
      "Accuracy with depth 10: 0.9366666666666666\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = clf_depth3.predict(x_test)\n",
    "#print(y_pred3)\n",
    "acc3 = Accuracy(y_test, y_pred3)\n",
    "print(\"Accuracy with depth 3: {}\".format(acc3))\n",
    "\n",
    "\n",
    "y_pred10 = clf_depth10.predict(x_test)\n",
    "acc10 = Accuracy(y_test, y_pred10)\n",
    "print(\"Accuracy with depth 10: {}\".format(acc10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "\n",
    "clf_gini.fit(x_train, y_train)\n",
    "clf_entropy.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with gini: 0.9166666666666666\n",
      "Accuracy with entropy: 0.93\n"
     ]
    }
   ],
   "source": [
    "y_pred_gini = clf_gini.predict(x_test)\n",
    "acc_gini = Accuracy(y_test, y_pred_gini)\n",
    "print(\"Accuracy with gini: {}\".format(acc_gini))\n",
    "\n",
    "\n",
    "y_pred_entropy = clf_entropy.predict(x_test)\n",
    "acc_entropy = Accuracy(y_test, y_pred_entropy)\n",
    "print(\"Accuracy with entropy: {}\".format(acc_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ram': 9, 'px_height': 9, 'fc': 2, 'battery_power': 19, 'px_width': 5, 'm_dep': 2, 'mobile_wt': 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# traverse the tree and count the features\n",
    "def calc_feature(node):\n",
    "    feature_dict = {}\n",
    "\n",
    "    if node.feature == -1 or node.feature is None:\n",
    "        return feature_dict\n",
    "\n",
    "    feature_dict[feature_names[node.feature]] = 1\n",
    "\n",
    "    if node.left is not None:\n",
    "        left = calc_feature(node.left)\n",
    "        for key in left.keys():\n",
    "            if key in feature_dict.keys():\n",
    "                feature_dict[key] += left[key]\n",
    "            else:\n",
    "                feature_dict[key] = left[key]\n",
    "\n",
    "    if node.right is not None:\n",
    "        right = calc_feature(node.right)\n",
    "        for key in right.keys():\n",
    "            if key in feature_dict.keys():\n",
    "                feature_dict[key] += right[key]\n",
    "            else:\n",
    "                feature_dict[key] = right[key]\n",
    "    \n",
    "    return feature_dict\n",
    "\n",
    "feature_dict = calc_feature(clf_depth10.root)\n",
    "print(feature_dict)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_dict.keys(), feature_dict.values())\n",
    "plt.xticks(rotation=30)\n",
    "sns.set()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakClassifier():\n",
    "    def __init__(self, cart=None, weight=None):\n",
    "        self.weight = weight\n",
    "        self.cart = cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "    def __init__(self, n_estimators, depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.classifiers = []\n",
    "        self.depth = depth\n",
    "\n",
    "    def fit(self, x_data, y_data):\n",
    "        weight = np.ones(shape=(x_data.shape[0]))\n",
    "        weight /= x_data.shape[0]  # initialize weight\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            cart = DecisionTree(criterion='gini', max_depth=self.depth)\n",
    "            weak_clf = WeakClassifier(cart=cart)\n",
    "\n",
    "            # sample new dataset depends on the weight\n",
    "            indices = np.random.choice(x_data.shape[0], x_data.shape[0], replace=True, p=weight)\n",
    "\n",
    "            x_samples = x_data[indices]\n",
    "            y_samples = y_data[indices]\n",
    "\n",
    "            cart.fit(x_samples, y_samples)\n",
    "            y_pred = cart.predict(x_data)\n",
    "\n",
    "            error = 0.0\n",
    "            for i in range(len(y_data)):\n",
    "                error += weight[i] * (y_data[i] != y_pred[i])\n",
    "            \n",
    "            # alpha\n",
    "            weak_clf.weight = (1/2) * np.log((1-error)/error)\n",
    "\n",
    "            # update weight of data\n",
    "            for i in range(len(weight)):\n",
    "                weight[i] = weight[i] * np.exp(-weak_clf.weight * (2*y_data[i]-1) * (2*y_pred[i]-1))\n",
    "\n",
    "            weight /= np.sum(weight)\n",
    "            self.classifiers.append(weak_clf)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        result = np.zeros(shape=x_data.shape[0])\n",
    "\n",
    "        for clf in self.classifiers:\n",
    "            y_pred = clf.cart.predict(x_data)\n",
    "            # the predictions are 0 and 1 -> transfer to -1 and 1\n",
    "            y_pred = 2 * y_pred - 1\n",
    "            for i in range(len(result)):\n",
    "                result[i] += clf.weight * y_pred[i]\n",
    "        \n",
    "        for i in range(len(result)):\n",
    "            if result[i] >= 0:\n",
    "                result[i] = 1\n",
    "            else:\n",
    "                result[i] = 0\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf10 = AdaBoost(n_estimators=10)\n",
    "clf10.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with n=10 : 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred_ada10 = clf10.predict(x_test)\n",
    "acc_ada10 = Accuracy(y_test, y_pred_ada10)\n",
    "print(\"Accuracy with n=10 : {}\".format(acc_ada10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf100 = AdaBoost(n_estimators=100)\n",
    "clf100.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with n=100 : 0.97\n"
     ]
    }
   ],
   "source": [
    "y_pred_ada100 = clf100.predict(x_test)\n",
    "acc_ada100 = Accuracy(y_test, y_pred_ada100)\n",
    "print(\"Accuracy with n=100 : {}\".format(acc_ada100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, bootstrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = math.ceil(max_features)\n",
    "        self.bootstrap = bootstrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.classifiers = []\n",
    "        \n",
    "    def fit(self, x_data, y_data):\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = x_data.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # sample from dataset\n",
    "            if self.bootstrap:\n",
    "                indices = np.random.choice(x_data.shape[0], x_data.shape[0], replace=True)\n",
    "            else:\n",
    "                indices = np.array([i for i in range(x_data.shape[0])])\n",
    "            #print(type(self.max_features))\n",
    "            \n",
    "            clf = DecisionTree(criterion='gini', max_depth=self.max_depth)\n",
    "            #print(indices, features)\n",
    "            clf.fit(x_data, y_data, indices=indices, max_features=self.max_features)\n",
    "\n",
    "            self.classifiers.append(clf)\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        result = np.zeros(shape=x_data.shape[0])\n",
    "\n",
    "        for i in range(x_data.shape[0]):\n",
    "            for clf in self.classifiers:\n",
    "                pred_cls = clf.traverse(x_data[i], clf.root)\n",
    "        \n",
    "                if pred_cls == 1:\n",
    "                    result[i] += 1\n",
    "                else:\n",
    "                    result[i] += -1\n",
    "        \n",
    "        for i in range(len(result)):\n",
    "            if result[i] >= 0:\n",
    "                result[i] = 1\n",
    "            else:\n",
    "                result[i] = 0\n",
    "        \n",
    "        return result\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest n=10 : 0.9433333333333334\n"
     ]
    }
   ],
   "source": [
    "clf_10tree.fit(x_train, y_train)\n",
    "pred_fore10 = clf_10tree.predict(x_test)\n",
    "acc_fore10 = Accuracy(y_test, pred_fore10)\n",
    "print(\"Accuracy of random forest n=10 : {}\".format(acc_fore10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest n=100 : 0.95\n"
     ]
    }
   ],
   "source": [
    "clf_100tree.fit(x_train, y_train)\n",
    "pred_fore100 = clf_100tree.predict(x_test)\n",
    "acc_fore100 = Accuracy(y_test, pred_fore100)\n",
    "print(\"Accuracy of random forest n=100 : {}\".format(acc_fore100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random feature: 0.93\n",
      "Accuracy of all feature: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "clf_random_features.fit(x_train, y_train)\n",
    "clf_all_features.fit(x_train, y_train)\n",
    "\n",
    "pred_rf = clf_random_features.predict(x_test)\n",
    "acc_rf = Accuracy(y_test, pred_rf)\n",
    "print(\"Accuracy of random feature: {}\".format(acc_rf))\n",
    "\n",
    "pred_af = clf_all_features.predict(x_test)\n",
    "acc_af = Accuracy(y_test, pred_af)\n",
    "print(\"Accuracy of all feature: {}\".format(acc_af))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6. Train and tune your model on a real-world dataset\n",
    "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_your_model(data):\n",
    "    ## Define your model and training\n",
    "    x_train = data.drop('price_range', axis=1)\n",
    "    x_train = x_train.values\n",
    "    y_train = data['price_range'].values\n",
    "\n",
    "    model = AdaBoost(n_estimators=150)\n",
    "    model.fit(x_train, y_train) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [train_df, val_df]\n",
    "data_df = pd.concat(frames)\n",
    "my_model = train_your_model(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('x_test.csv')\n",
    "x_test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = my_model.predict(x_test)\n",
    "np.save('y_pred.npy', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_pred.shape == (500, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT MODIFY CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
    "\n",
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "        return score\n",
    "    else:\n",
    "        print(f\"{name} failed\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def patient_checker(score, thres, CLS, kwargs, name,\n",
    "                    x_train, y_train, x_test, y_test, patient=10):\n",
    "    while patient > 0:\n",
    "        patient -= 1\n",
    "        clf = CLS(**kwargs)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        \n",
    "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
    "            return score\n",
    "    print(f\"{name} failed\")\n",
    "    print(\"Considering the randomness, we will check it manually\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
    "    df = pd.read_csv(\n",
    "        file_url,\n",
    "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
    "    )\n",
    "\n",
    "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
    "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
    "\n",
    "    train_idx = range(0, len(df), 10)\n",
    "    test_idx = range(1, len(df), 20)\n",
    "\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "\n",
    "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    feature_names = x_train.columns.values\n",
    "    x_train = x_train.values\n",
    "    y_train = train_df['Target'].values\n",
    "\n",
    "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
    "    x_test = x_test.values\n",
    "    y_test = test_df['Target'].values\n",
    "    return x_train, y_train, x_test, y_test, feature_names\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "data = np.array([1, 2])\n",
    "if abs(gini(data) - 0.5) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"gini test failed\")\n",
    "\n",
    "if abs(entropy(data) - 1) < 1e-4:\n",
    "    score += 2.5\n",
    "else:\n",
    "    print(\"entropy test failed\")\n",
    "\n",
    "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
    "\n",
    "score += discrete_checker(5, 0.9337,\n",
    "                          DecisionTree(criterion='gini', max_depth=3),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9036,\n",
    "                          DecisionTree(criterion='gini', max_depth=10),\n",
    "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "score += discrete_checker(2.5, 0.9096,\n",
    "                          DecisionTree(criterion='entropy', max_depth=3),\n",
    "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
    "                          x_train, y_train, x_test, y_test\n",
    "                          )\n",
    "\n",
    "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
    "    \"AdaBoost(n_estimators=10)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
    "    \"AdaBoost(n_estimators=100)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.91, RandomForest,\n",
    "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
    "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "score += patient_checker(\n",
    "    5, 0.92, RandomForest,\n",
    "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
    "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
    "    x_train, y_train, x_test, y_test\n",
    ")\n",
    "\n",
    "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
    "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
    "print(\"*** This score is only for reference ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9c6da53ade2302c5c2549050a755f7271f56c4989f0bdbb18e4ce6e64f091b09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
